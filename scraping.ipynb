{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_pdf_table(df):\n",
    "    \"\"\"\n",
    "    Fix common PDF table extraction problems:\n",
    "    - Header split into multiple rows\n",
    "    - Header split across multiple columns\n",
    "    - First row mistakenly treated as data\n",
    "    - Missing/partial header fragments\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove whitespace everywhere\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # If columns are numbers, use row 0 as header\n",
    "    if all(isinstance(c, int) for c in df.columns):\n",
    "        if df.iloc[0].apply(lambda x: isinstance(x, str)).mean() > 0.6:\n",
    "            df.columns = df.iloc[0]\n",
    "            df = df[1:].reset_index(drop=True)\n",
    "            return df\n",
    "\n",
    "    # If row 0 is mostly empty => split header\n",
    "    if df.iloc[0].isna().sum() > len(df.columns) / 2:\n",
    "        header = (df.iloc[0].fillna('') + \" \" + df.iloc[1].fillna('')).str.strip()\n",
    "        df.columns = header\n",
    "        df = df[2:].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    # If row 0 contains fragments\n",
    "    if df.iloc[0].astype(str).str.len().mean() < 5:\n",
    "        header = (df.iloc[0].fillna('') + \" \" + df.iloc[1].fillna('')).str.strip()\n",
    "        df.columns = header\n",
    "        df = df[2:].reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    # Fallback\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. PARSE PDF AND CLEAN TABLES\n",
    "# ------------------------------------------------------------\n",
    "def extract_pdf_tables(pdf_path):\n",
    "    all_tables = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "\n",
    "            tables = page.extract_tables()\n",
    "\n",
    "            if not tables:\n",
    "                continue\n",
    "\n",
    "            for tbl in tables:\n",
    "                raw_df = pd.DataFrame(tbl)\n",
    "                clean_df = fix_pdf_table(raw_df)\n",
    "                all_tables.append(clean_df)\n",
    "\n",
    "    return all_tables\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. MERGE ALL TABLES (OPTIONAL)\n",
    "# ------------------------------------------------------------\n",
    "def merge_tables(table_list):\n",
    "    \"\"\"\n",
    "    Only use this if ALL tables have identical columns.\n",
    "    \"\"\"\n",
    "    return pd.concat(table_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"northmet-feis-adequacy-exhibit-a.pdf\"   # <-- change this\n",
    "\n",
    "tables = extract_pdf_tables(pdf_file)\n",
    "\n",
    "print(f\"\\nExtracted and cleaned {len(tables)} tables.\\n\")\n",
    "\n",
    "# Example: show first cleaned table\n",
    "if tables:\n",
    "    print(tables[0].head())\n",
    "\n",
    "merged = merge_tables(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Substantive /</th>\n",
       "      <th>Old /</th>\n",
       "      <th>Response</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sender</td>\n",
       "      <td>Comment</td>\n",
       "      <td>Issue</td>\n",
       "      <td>Non-Substantive</td>\n",
       "      <td>New</td>\n",
       "      <td>ID</td>\n",
       "      <td>RGU Consideration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kathleen\\nWhitson</td>\n",
       "      <td>PLEASE do NOT approve the mining. It will prof...</td>\n",
       "      <td>GEN</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mark</td>\n",
       "      <td>The environment will eventually be polluted by...</td>\n",
       "      <td>FIN</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob\\nWoodbury</td>\n",
       "      <td>Have there been other projects of this nature ...</td>\n",
       "      <td>PER</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob\\nWoodbury</td>\n",
       "      <td>I could go on in this vein, but my point is th...</td>\n",
       "      <td>PER</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0            Name of                                                     \\\n",
       "0             Sender                                            Comment   \n",
       "1  Kathleen\\nWhitson  PLEASE do NOT approve the mining. It will prof...   \n",
       "2               Mark  The environment will eventually be polluted by...   \n",
       "3      Bob\\nWoodbury  Have there been other projects of this nature ...   \n",
       "4      Bob\\nWoodbury  I could go on in this vein, but my point is th...   \n",
       "\n",
       "0           Substantive / Old / Response                     \n",
       "0  Issue  Non-Substantive   New       ID  RGU Consideration  \n",
       "1    GEN               NS     X        1                     \n",
       "2    FIN               NS     X        1                     \n",
       "3    PER               NS     X        1                     \n",
       "4    PER               NS     X        1                     "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name of Sender', ' Comment', ' Issue', 'Substantive / Non-Substantive', 'Old / New', 'Response ID', ' RGU Consideration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/k0g9brfx65x_m7n7bqdpmlhm0000gn/T/ipykernel_46317/3150135633.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  ret_columns.append(merged.columns[i] + ' ' + merged.iloc[0][i])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Sender</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Substantive / Non-Substantive</th>\n",
       "      <th>Old / New</th>\n",
       "      <th>Response ID</th>\n",
       "      <th>RGU Consideration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kathleen\\nWhitson</td>\n",
       "      <td>PLEASE do NOT approve the mining. It will prof...</td>\n",
       "      <td>GEN</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mark</td>\n",
       "      <td>The environment will eventually be polluted by...</td>\n",
       "      <td>FIN</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob\\nWoodbury</td>\n",
       "      <td>Have there been other projects of this nature ...</td>\n",
       "      <td>PER</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bob\\nWoodbury</td>\n",
       "      <td>I could go on in this vein, but my point is th...</td>\n",
       "      <td>PER</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bob\\nWoodbury</td>\n",
       "      <td>We need to make a decision on what we know, no...</td>\n",
       "      <td>NEPA</td>\n",
       "      <td>NS</td>\n",
       "      <td>X</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name of Sender                                            Comment  \\\n",
       "0  Kathleen\\nWhitson  PLEASE do NOT approve the mining. It will prof...   \n",
       "1               Mark  The environment will eventually be polluted by...   \n",
       "2      Bob\\nWoodbury  Have there been other projects of this nature ...   \n",
       "3      Bob\\nWoodbury  I could go on in this vein, but my point is th...   \n",
       "4      Bob\\nWoodbury  We need to make a decision on what we know, no...   \n",
       "\n",
       "   Issue Substantive / Non-Substantive Old / New Response ID  \\\n",
       "0    GEN                            NS         X           1   \n",
       "1    FIN                            NS         X           1   \n",
       "2    PER                            NS         X           1   \n",
       "3    PER                            NS         X           1   \n",
       "4   NEPA                            NS         X           1   \n",
       "\n",
       "   RGU Consideration  \n",
       "0                     \n",
       "1                     \n",
       "2                     \n",
       "3                     \n",
       "4                     "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_columns = []\n",
    "for i in range(len(merged.columns)):\n",
    "    ret_columns.append(merged.columns[i] + ' ' + merged.iloc[0][i])\n",
    "\n",
    "merged.columns = ret_columns\n",
    "\n",
    "merged = merged[1:].reset_index(drop=True)\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(\"northmet-feis-adequacy-exhibit-a.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Environmental Impact Statement (FEIS)\n",
      "NorthMet Mining Project and Land Exchange\n",
      "Table A-3 Cooperating Agency Comments and Responses\n",
      "Comment ID Comment Response Theme(s)\n",
      "Comments from the USEPA (Submission ID 47834)\n",
      "2981 Comment # 1. Spill prevention is an important part To guard against possible adverse effects from spilled ore, PolyMet plans WR 151\n",
      "of the mitigation for this project. Using new or to refurbish the ore cars, tightening or replacing the couplings and linkages\n",
      "retrofit side dump rail cars (possibly with to minimize gaps along the hinges and joint areas where spillage would\n",
      "hydraulic air-operation conversions) should be occur. The quantity of ore that could potentially spill through the door and\n",
      "considered as part of the mitigation package for hinge gaps of a refurbished ore car is estimated to be 0.20 ton per year. This\n",
      "the proposed action. Proactive mitigation through is a 97 percent reduction from the originally calculated value of 6.14 tons\n",
      "the use of updated rail infrastructure would help per year.\n",
      "reduce spillage and subsequent environmental Water quality monitoring is identified downstream from the rail line on the\n",
      "concerns, possibly including the need for Partridge River tributary streams to check for any potential deteriorations\n",
      "additional long-term water treatment. of water quality over time from ore spillage, and, if detected, adaptive\n",
      "Recommendation: Consider use of new or retrofit water management measures would be implemented. Dust could be\n",
      "side-dump rail cars when producing the spilled ore mitigated by spraying water on the loaded ore prior to transport. If\n",
      "plan. significant accumulation of ore spillage occurs, it would be removed. The\n",
      "Permit to Mine would further address rail cars design in a section titled Ore\n",
      "Management, Handling and Transport.\n",
      "2982 Comment # 2. Pages 5-50 forward describe how Mine waste rock would be sorted and stored into four categories based on WR 027\n",
      "the company has classified its waste rock and its sulfur content. Category 1 waste rock would not produce acid leachate.\n",
      "tailings into four categories based on their Category 2/3 waste rock may produce acid leachate if allowed to weather\n",
      "likelihood to generate acid rock drainage. We for several years. Category 4 waste rock would produce acidic leachate if\n",
      "understand from discussion with the co-lead allowed to weather for several years. Category 1 waste rock would be\n",
      "agencies that lime will be added to Category l stored in a permanent stockpile that would be encompassed by a water\n",
      "waste rock, which is expected to result in neutral containment system to capture surface water and groundwater from the\n",
      "to slightly basic pH. stockpile and direct it to a water treatment facility and would have a\n",
      "Recommendation: The FEIS should indicate that geomembrane cover at closure. Because Category 1 waste rock would not\n",
      "Category I waste rock leachate is expected to have generate acid, and because water from the stockpile would be captured and\n",
      "a neutral to slightly basic pH due to the addition of treated, lime is not anticipated to be needed for neutralization, and,\n",
      "lime. therefore, the addition of lime for Category 1 waste rock is not proposed.\n",
      "Category 2/3 and Category 4 waste rock would be stored temporarily in\n",
      "lined stockpiles, and then backfilled into the East Pit following completion\n",
      "of mining there. Lime may be added to the waste rock during East Pit\n",
      "backfilling to maintain pH in the pit pore water as needed. The volume of\n",
      "lime required would be based on monitoring results. Waste rock\n",
      "characterization and categorization, as well as management and storage\n",
      "APPENDIX A: RESPONSE TO COMMENTS ON THE DEIS AND SDEIS A-8 NOVEMBER 2015\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"014_appendix_a_response_to_comments_on_the_NorthMet_EIS.pdf\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    page = pdf.pages[9]    # or any table page\n",
    "    print(page.extract_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9: Extracted 29 rows, 1 columns.\n",
      "Page 10: Extracted 36 rows, 1 columns.\n",
      "Page 11: Extracted 39 rows, 1 columns.\n",
      "Page 12: Extracted 40 rows, 1 columns.\n",
      "Page 13: Extracted 39 rows, 1 columns.\n",
      "Page 14: Extracted 40 rows, 1 columns.\n",
      "Page 15: Extracted 38 rows, 1 columns.\n",
      "Page 16: Extracted 39 rows, 1 columns.\n",
      "Page 17: Extracted 39 rows, 1 columns.\n",
      "Page 18: Extracted 41 rows, 1 columns.\n",
      "Page 19: Extracted 39 rows, 1 columns.\n",
      "Page 20: Extracted 38 rows, 1 columns.\n",
      "Page 21: Extracted 65 rows, 1 columns.\n",
      "Page 22: Extracted 38 rows, 1 columns.\n",
      "Page 23: Extracted 40 rows, 1 columns.\n",
      "Page 24: Extracted 39 rows, 1 columns.\n",
      "Page 25: Extracted 39 rows, 1 columns.\n",
      "Page 26: Extracted 39 rows, 1 columns.\n",
      "Page 27: Extracted 39 rows, 1 columns.\n",
      "Page 28: Extracted 39 rows, 1 columns.\n",
      "Page 29: Extracted 39 rows, 1 columns.\n",
      "Page 30: Extracted 39 rows, 1 columns.\n",
      "Page 31: Extracted 37 rows, 1 columns.\n",
      "Page 32: Extracted 38 rows, 1 columns.\n",
      "Page 33: Extracted 38 rows, 1 columns.\n",
      "Page 34: Extracted 39 rows, 1 columns.\n",
      "Page 35: Extracted 44 rows, 1 columns.\n",
      "Page 36: Extracted 38 rows, 1 columns.\n",
      "Page 37: Extracted 39 rows, 1 columns.\n",
      "Page 38: Extracted 27 rows, 1 columns.\n",
      "Page 39: Extracted 39 rows, 1 columns.\n",
      "Page 40: Extracted 39 rows, 1 columns.\n",
      "Page 41: Extracted 40 rows, 1 columns.\n",
      "Page 42: Extracted 39 rows, 1 columns.\n",
      "Page 43: Extracted 39 rows, 1 columns.\n",
      "Page 44: Extracted 38 rows, 1 columns.\n",
      "Page 45: Extracted 38 rows, 1 columns.\n",
      "Page 46: Extracted 38 rows, 1 columns.\n",
      "Page 47: Extracted 39 rows, 1 columns.\n",
      "Page 48: Extracted 10 rows, 1 columns.\n",
      "Page 49: Extracted 39 rows, 1 columns.\n",
      "Page 50: Extracted 47 rows, 1 columns.\n",
      "Page 51: Extracted 39 rows, 1 columns.\n",
      "Page 52: Extracted 39 rows, 1 columns.\n",
      "Page 53: Extracted 39 rows, 1 columns.\n",
      "Page 54: Extracted 39 rows, 1 columns.\n",
      "Page 55: Extracted 38 rows, 1 columns.\n",
      "Page 56: Extracted 39 rows, 1 columns.\n",
      "Page 57: Extracted 39 rows, 1 columns.\n",
      "Page 58: Extracted 39 rows, 1 columns.\n",
      "Page 59: Extracted 39 rows, 1 columns.\n",
      "Page 60: Extracted 39 rows, 1 columns.\n",
      "Page 61: Extracted 39 rows, 1 columns.\n",
      "Page 62: Extracted 38 rows, 1 columns.\n",
      "Page 63: Extracted 39 rows, 1 columns.\n",
      "Page 64: Extracted 39 rows, 1 columns.\n",
      "Page 65: Extracted 39 rows, 1 columns.\n",
      "Page 66: Extracted 39 rows, 1 columns.\n",
      "Page 67: Extracted 39 rows, 1 columns.\n",
      "Page 68: Extracted 39 rows, 1 columns.\n",
      "Page 69: Extracted 39 rows, 1 columns.\n",
      "Page 70: Extracted 39 rows, 1 columns.\n",
      "Page 71: Extracted 40 rows, 1 columns.\n",
      "Page 72: Extracted 39 rows, 1 columns.\n",
      "Page 73: Extracted 42 rows, 1 columns.\n",
      "Page 74: Extracted 44 rows, 1 columns.\n",
      "Page 75: Extracted 39 rows, 1 columns.\n",
      "Page 76: Extracted 39 rows, 1 columns.\n",
      "Page 77: Extracted 42 rows, 1 columns.\n",
      "Page 78: Extracted 40 rows, 1 columns.\n",
      "Page 79: Extracted 39 rows, 1 columns.\n",
      "Page 80: Extracted 39 rows, 1 columns.\n",
      "Page 81: Extracted 48 rows, 1 columns.\n",
      "Page 82: Extracted 49 rows, 1 columns.\n",
      "Page 83: Extracted 43 rows, 1 columns.\n",
      "Page 84: Extracted 44 rows, 1 columns.\n",
      "Page 85: Extracted 40 rows, 1 columns.\n",
      "Page 86: Extracted 40 rows, 1 columns.\n",
      "Page 87: Extracted 39 rows, 1 columns.\n",
      "Page 88: Extracted 40 rows, 1 columns.\n",
      "Page 89: Extracted 39 rows, 1 columns.\n",
      "Page 90: Extracted 38 rows, 1 columns.\n",
      "Page 91: Extracted 39 rows, 1 columns.\n",
      "Page 92: Extracted 39 rows, 1 columns.\n",
      "Page 93: Extracted 39 rows, 1 columns.\n",
      "Page 94: Extracted 39 rows, 1 columns.\n",
      "Page 95: Extracted 39 rows, 1 columns.\n",
      "Page 96: Extracted 39 rows, 1 columns.\n",
      "Page 97: Extracted 48 rows, 1 columns.\n",
      "Page 98: Extracted 39 rows, 1 columns.\n",
      "Page 99: Extracted 39 rows, 1 columns.\n",
      "Page 100: Extracted 41 rows, 1 columns.\n",
      "Page 101: Extracted 40 rows, 1 columns.\n",
      "Page 102: Extracted 39 rows, 1 columns.\n",
      "Page 103: Extracted 39 rows, 1 columns.\n",
      "Page 104: Extracted 39 rows, 1 columns.\n",
      "Page 105: Extracted 39 rows, 1 columns.\n",
      "Page 106: Extracted 39 rows, 1 columns.\n",
      "Page 107: Extracted 53 rows, 1 columns.\n",
      "Page 108: Extracted 36 rows, 1 columns.\n",
      "Page 109: Extracted 39 rows, 1 columns.\n",
      "Page 110: Extracted 39 rows, 1 columns.\n",
      "Page 111: Extracted 40 rows, 1 columns.\n",
      "Page 112: Extracted 48 rows, 1 columns.\n",
      "Page 113: Extracted 42 rows, 1 columns.\n",
      "Page 114: Extracted 50 rows, 1 columns.\n",
      "Page 115: Extracted 49 rows, 1 columns.\n",
      "Page 116: Extracted 43 rows, 1 columns.\n",
      "Page 117: Extracted 39 rows, 1 columns.\n",
      "Page 118: Extracted 39 rows, 1 columns.\n",
      "Page 119: Extracted 40 rows, 1 columns.\n",
      "Page 120: Extracted 37 rows, 1 columns.\n",
      "Page 121: Extracted 39 rows, 1 columns.\n",
      "Page 122: Extracted 47 rows, 1 columns.\n",
      "Page 123: Extracted 38 rows, 1 columns.\n",
      "Page 124: Extracted 39 rows, 1 columns.\n",
      "Page 125: Extracted 39 rows, 1 columns.\n",
      "Page 126: Extracted 38 rows, 1 columns.\n",
      "Page 127: Extracted 39 rows, 1 columns.\n",
      "Page 128: Extracted 39 rows, 1 columns.\n",
      "Page 129: Extracted 39 rows, 1 columns.\n",
      "Page 130: Extracted 39 rows, 1 columns.\n",
      "Page 131: Extracted 9 rows, 1 columns.\n",
      "Page 132: Extracted 44 rows, 1 columns.\n",
      "Page 133: Extracted 38 rows, 1 columns.\n",
      "Page 134: Extracted 39 rows, 1 columns.\n",
      "Page 135: Extracted 39 rows, 1 columns.\n",
      "Page 136: Extracted 39 rows, 1 columns.\n",
      "Page 137: Extracted 38 rows, 1 columns.\n",
      "Page 138: Extracted 51 rows, 1 columns.\n",
      "Page 139: Extracted 39 rows, 1 columns.\n",
      "Page 140: Extracted 39 rows, 1 columns.\n",
      "Page 141: Extracted 39 rows, 1 columns.\n",
      "Page 142: Extracted 39 rows, 1 columns.\n",
      "Page 143: Extracted 42 rows, 1 columns.\n",
      "Page 144: Extracted 43 rows, 1 columns.\n",
      "Page 145: Extracted 40 rows, 1 columns.\n",
      "Page 146: Extracted 48 rows, 1 columns.\n",
      "Page 147: Extracted 39 rows, 1 columns.\n",
      "Page 148: Extracted 39 rows, 1 columns.\n",
      "Page 149: Extracted 39 rows, 1 columns.\n",
      "Page 150: Extracted 43 rows, 1 columns.\n",
      "Page 151: Extracted 39 rows, 1 columns.\n",
      "Page 152: Extracted 39 rows, 1 columns.\n",
      "Page 153: Extracted 39 rows, 1 columns.\n",
      "Page 154: Extracted 39 rows, 1 columns.\n",
      "Page 155: Extracted 39 rows, 1 columns.\n",
      "Page 156: Extracted 39 rows, 1 columns.\n",
      "Page 157: Extracted 38 rows, 1 columns.\n",
      "Page 158: Extracted 39 rows, 1 columns.\n",
      "Page 159: Extracted 40 rows, 1 columns.\n",
      "Page 160: Extracted 39 rows, 1 columns.\n",
      "Page 161: Extracted 39 rows, 1 columns.\n",
      "Page 162: Extracted 39 rows, 1 columns.\n",
      "Page 163: Extracted 40 rows, 1 columns.\n",
      "Page 164: Extracted 39 rows, 1 columns.\n",
      "Page 165: Extracted 39 rows, 1 columns.\n",
      "Page 166: Extracted 39 rows, 1 columns.\n",
      "Page 167: Extracted 39 rows, 1 columns.\n",
      "Page 168: Extracted 39 rows, 1 columns.\n",
      "Page 169: Extracted 39 rows, 1 columns.\n",
      "Page 170: Extracted 39 rows, 1 columns.\n",
      "Page 171: Extracted 40 rows, 1 columns.\n",
      "Page 172: Extracted 39 rows, 1 columns.\n",
      "Page 173: Extracted 40 rows, 1 columns.\n",
      "Page 174: Extracted 40 rows, 1 columns.\n",
      "Page 175: Extracted 40 rows, 1 columns.\n",
      "Page 176: Extracted 39 rows, 1 columns.\n",
      "Page 177: Extracted 39 rows, 1 columns.\n",
      "Page 178: Extracted 39 rows, 1 columns.\n",
      "Page 179: Extracted 36 rows, 1 columns.\n",
      "Page 180: Extracted 16 rows, 1 columns.\n",
      "Page 181: Extracted 34 rows, 1 columns.\n",
      "Page 182: Extracted 51 rows, 1 columns.\n",
      "Page 183: Extracted 39 rows, 1 columns.\n",
      "Page 184: Extracted 39 rows, 1 columns.\n",
      "Page 185: Extracted 52 rows, 1 columns.\n",
      "Page 186: Extracted 40 rows, 1 columns.\n",
      "Page 187: Extracted 39 rows, 1 columns.\n",
      "Page 188: Extracted 39 rows, 1 columns.\n",
      "Page 189: Extracted 39 rows, 1 columns.\n",
      "Page 190: Extracted 45 rows, 1 columns.\n",
      "Page 191: Extracted 38 rows, 1 columns.\n",
      "Page 192: Extracted 46 rows, 1 columns.\n",
      "Page 193: Extracted 38 rows, 1 columns.\n",
      "Page 194: Extracted 43 rows, 1 columns.\n",
      "Page 195: Extracted 39 rows, 1 columns.\n",
      "Page 196: Extracted 46 rows, 1 columns.\n",
      "Page 197: Extracted 39 rows, 1 columns.\n",
      "Page 198: Extracted 39 rows, 1 columns.\n",
      "Page 199: Extracted 39 rows, 1 columns.\n",
      "Page 200: Extracted 39 rows, 1 columns.\n",
      "Page 201: Extracted 39 rows, 1 columns.\n",
      "Page 202: Extracted 39 rows, 1 columns.\n",
      "Page 203: Extracted 39 rows, 1 columns.\n",
      "Page 204: Extracted 42 rows, 1 columns.\n",
      "Page 205: Extracted 39 rows, 1 columns.\n",
      "Page 206: Extracted 39 rows, 1 columns.\n",
      "Page 207: Extracted 49 rows, 1 columns.\n",
      "Page 208: Extracted 46 rows, 1 columns.\n",
      "Page 209: Extracted 44 rows, 1 columns.\n",
      "Page 210: Extracted 46 rows, 1 columns.\n",
      "Page 211: Extracted 39 rows, 1 columns.\n",
      "Page 212: Extracted 38 rows, 1 columns.\n",
      "Page 213: Extracted 39 rows, 1 columns.\n",
      "Page 214: Extracted 38 rows, 1 columns.\n",
      "Page 215: Extracted 39 rows, 1 columns.\n",
      "Page 216: Extracted 39 rows, 1 columns.\n",
      "Page 217: Extracted 40 rows, 1 columns.\n",
      "Page 218: Extracted 40 rows, 1 columns.\n",
      "Page 219: Extracted 38 rows, 1 columns.\n",
      "Page 220: Extracted 38 rows, 1 columns.\n",
      "Page 221: Extracted 38 rows, 1 columns.\n",
      "Page 222: Extracted 38 rows, 1 columns.\n",
      "Page 223: Extracted 39 rows, 1 columns.\n",
      "Page 224: Extracted 37 rows, 1 columns.\n",
      "Page 225: Extracted 38 rows, 1 columns.\n",
      "Page 226: Extracted 38 rows, 1 columns.\n",
      "Page 227: Extracted 39 rows, 1 columns.\n",
      "Page 228: Extracted 45 rows, 1 columns.\n",
      "Page 229: Extracted 44 rows, 1 columns.\n",
      "Page 230: Extracted 39 rows, 1 columns.\n",
      "Page 231: Extracted 38 rows, 1 columns.\n",
      "Page 232: Extracted 39 rows, 1 columns.\n",
      "Page 233: Extracted 39 rows, 1 columns.\n",
      "Page 234: Extracted 39 rows, 1 columns.\n",
      "Page 235: Extracted 39 rows, 1 columns.\n",
      "Page 236: Extracted 38 rows, 1 columns.\n",
      "Page 237: Extracted 40 rows, 1 columns.\n",
      "Page 238: Extracted 39 rows, 1 columns.\n",
      "Page 239: Extracted 38 rows, 1 columns.\n",
      "Page 240: Extracted 53 rows, 1 columns.\n",
      "Page 241: Extracted 39 rows, 1 columns.\n",
      "Page 242: Extracted 39 rows, 1 columns.\n",
      "Page 243: Extracted 39 rows, 1 columns.\n",
      "Page 244: Extracted 39 rows, 1 columns.\n",
      "Page 245: Extracted 50 rows, 1 columns.\n",
      "Page 246: Extracted 39 rows, 1 columns.\n",
      "Page 247: Extracted 39 rows, 1 columns.\n",
      "Page 248: Extracted 38 rows, 1 columns.\n",
      "Page 249: Extracted 38 rows, 1 columns.\n",
      "Page 250: Extracted 39 rows, 1 columns.\n",
      "Page 251: Extracted 10 rows, 1 columns.\n",
      "Page 252: Extracted 38 rows, 1 columns.\n",
      "Page 253: Extracted 39 rows, 1 columns.\n",
      "Page 254: Extracted 39 rows, 1 columns.\n",
      "Page 255: Extracted 39 rows, 1 columns.\n",
      "Page 256: Extracted 39 rows, 1 columns.\n",
      "Page 257: Extracted 39 rows, 1 columns.\n",
      "Page 258: Extracted 39 rows, 1 columns.\n",
      "Page 259: Extracted 39 rows, 1 columns.\n",
      "Page 260: Extracted 38 rows, 1 columns.\n",
      "Page 261: Extracted 39 rows, 1 columns.\n",
      "Page 262: Extracted 40 rows, 1 columns.\n",
      "Page 263: Extracted 41 rows, 1 columns.\n",
      "Page 264: Extracted 40 rows, 1 columns.\n",
      "Page 265: Extracted 40 rows, 1 columns.\n",
      "Page 266: Extracted 40 rows, 1 columns.\n",
      "Page 267: Extracted 45 rows, 1 columns.\n",
      "Page 268: Extracted 39 rows, 1 columns.\n",
      "Page 269: Extracted 46 rows, 1 columns.\n",
      "Page 270: Extracted 45 rows, 1 columns.\n",
      "Page 271: Extracted 43 rows, 1 columns.\n",
      "Page 272: Extracted 57 rows, 1 columns.\n",
      "Page 273: Extracted 39 rows, 1 columns.\n",
      "Page 274: Extracted 39 rows, 1 columns.\n",
      "Page 275: Extracted 42 rows, 1 columns.\n",
      "Page 276: Extracted 38 rows, 1 columns.\n",
      "Page 277: Extracted 39 rows, 1 columns.\n",
      "Page 278: Extracted 39 rows, 1 columns.\n",
      "Page 279: Extracted 40 rows, 1 columns.\n",
      "Page 280: Extracted 39 rows, 1 columns.\n",
      "Page 281: Extracted 29 rows, 1 columns.\n",
      "Page 282: Extracted 39 rows, 1 columns.\n",
      "Page 283: Extracted 39 rows, 1 columns.\n",
      "Page 284: Extracted 44 rows, 1 columns.\n",
      "Page 285: Extracted 39 rows, 1 columns.\n",
      "Page 286: Extracted 39 rows, 1 columns.\n",
      "Page 287: Extracted 39 rows, 1 columns.\n",
      "Page 288: Extracted 44 rows, 1 columns.\n",
      "Page 289: Extracted 39 rows, 1 columns.\n",
      "Page 290: Extracted 39 rows, 1 columns.\n",
      "Page 291: Extracted 39 rows, 1 columns.\n",
      "Page 292: Extracted 39 rows, 1 columns.\n",
      "Page 293: Extracted 37 rows, 1 columns.\n",
      "Page 294: Extracted 39 rows, 1 columns.\n",
      "Page 295: Extracted 39 rows, 1 columns.\n",
      "Page 296: Extracted 39 rows, 1 columns.\n",
      "Page 297: Extracted 39 rows, 1 columns.\n",
      "Page 298: Extracted 37 rows, 1 columns.\n",
      "Page 299: Extracted 39 rows, 1 columns.\n",
      "Page 300: Extracted 39 rows, 1 columns.\n",
      "Page 301: Extracted 38 rows, 1 columns.\n",
      "Page 302: Extracted 39 rows, 1 columns.\n",
      "Page 303: Extracted 39 rows, 1 columns.\n",
      "Page 304: Extracted 36 rows, 1 columns.\n",
      "\n",
      "Merged table shape: (11726, 1)\n",
      "\n",
      "Saved cleaned table to: NorthMet_FEIS_Table_A3_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "PDF_PATH = \"yourfile.pdf\"\n",
    "START_PAGE = 10\n",
    "END_PAGE = 304\n",
    "\n",
    "# Strings to remove from every page\n",
    "REMOVE_HEADERS = [\n",
    "    \"Final Environmental Impact Statement (FEIS)\",\n",
    "    \"NorthMet Mining Project and Land Exchange\",\n",
    "]\n",
    "\n",
    "# Strings to remove only on first page\n",
    "FIRST_PAGE_REMOVE = [\n",
    "    \"Table A-3 Cooperating Agency Comments and Responses\",\n",
    "    \"Comments from the USEPA (Submission ID 47834)\",\n",
    "]\n",
    "\n",
    "# Regex to detect the true start of a new row\n",
    "COMMENT_ID_REGEX = r\"^\\d{4}$\"\n",
    "\n",
    "# Columns based on observed layout\n",
    "COLUMN_BOUNDS = {\n",
    "    \"Comment ID\": (0, 120),\n",
    "    \"Comment\": (120, 900),\n",
    "    \"Response\": (900, 1700),\n",
    "    \"Theme(s)\": (1700, 2300),\n",
    "}\n",
    "\n",
    "\n",
    "def clean_lines(lines, first_page=False):\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        if line in REMOVE_HEADERS:\n",
    "            continue\n",
    "        if first_page and line in FIRST_PAGE_REMOVE:\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def extract_words_by_column(words):\n",
    "    \"\"\"\n",
    "    Assigns each PDF word to a column based on x0 (left position).\n",
    "    \"\"\"\n",
    "    col_data = {col: [] for col in COLUMN_BOUNDS}\n",
    "    for w in words:\n",
    "        x0 = w[\"x0\"]\n",
    "        text = w[\"text\"]\n",
    "        for col, (xmin, xmax) in COLUMN_BOUNDS.items():\n",
    "            if xmin <= x0 < xmax:\n",
    "                col_data[col].append((w[\"top\"], text))  # Keep y-position to preserve order\n",
    "                break\n",
    "    return col_data\n",
    "\n",
    "\n",
    "def parse_page(page, first_page=False):\n",
    "    \"\"\"\n",
    "    Extracts structured rows from a single page.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract text raw for header removal\n",
    "    raw_lines = page.extract_text().split(\"\\n\")\n",
    "    lines = clean_lines(raw_lines, first_page=first_page)\n",
    "\n",
    "    # Now extract word-level positions\n",
    "    words = page.extract_words(use_text_flow=True)\n",
    "\n",
    "    # Group words into columns using x-positions\n",
    "    col_words = extract_words_by_column(words)\n",
    "\n",
    "    # Convert each column's word list into sorted text blocks\n",
    "    col_text = {}\n",
    "    for col, items in col_words.items():\n",
    "        # Sort by y-position (top)\n",
    "        items_sorted = sorted(items, key=lambda x: x[0])\n",
    "        # Extract just words\n",
    "        text = \" \".join([t for _, t in items_sorted])\n",
    "        col_text[col] = text\n",
    "\n",
    "    return col_text\n",
    "\n",
    "\n",
    "def reconstruct_rows(col_text):\n",
    "    \"\"\"\n",
    "    Reconstructs actual table rows by splitting on Comment ID occurrences.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cell_ids = re.findall(COMMENT_ID_REGEX, col_text[\"Comment ID\"])\n",
    "\n",
    "    if not cell_ids:\n",
    "        return rows\n",
    "\n",
    "    # Split Comment ID column into rows\n",
    "    cid_list = re.split(r\"\\s+\", col_text[\"Comment ID\"])\n",
    "    comment_list = re.split(r\"(?=Comment #)\", col_text[\"Comment\"])\n",
    "    response_list = re.split(r\"(?=FEIS|Mine|Water|During|This|\\w+)\", col_text[\"Response\"])\n",
    "    theme_list = col_text[\"Theme(s)\"].split()\n",
    "\n",
    "    # Align lengths safely\n",
    "    max_len = max(len(cid_list), len(comment_list), len(response_list), len(theme_list))\n",
    "    cid_list += [\"\"] * (max_len - len(cid_list))\n",
    "    comment_list += [\"\"] * (max_len - len(comment_list))\n",
    "    response_list += [\"\"] * (max_len - len(response_list))\n",
    "    theme_list += [\"\"] * (max_len - len(theme_list))\n",
    "\n",
    "    for i in range(max_len):\n",
    "        rows.append([\n",
    "            cid_list[i].strip(),\n",
    "            comment_list[i].strip(),\n",
    "            response_list[i].strip(),\n",
    "            theme_list[i].strip()\n",
    "        ])\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN EXTRACTION LOOP\n",
    "# -------------------------------\n",
    "\n",
    "all_rows = []\n",
    "first = True\n",
    "\n",
    "with pdfplumber.open(PDF_PATH) as pdf:\n",
    "    for i in range(START_PAGE - 1, END_PAGE):\n",
    "        page = pdf.pages[i]\n",
    "\n",
    "        col_text = parse_page(page, first_page=first)\n",
    "        first = False\n",
    "\n",
    "        rows = reconstruct_rows(col_text)\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "# -------------------------------\n",
    "# BUILD FINAL DATAFRAME\n",
    "# -------------------------------\n",
    "\n",
    "df = pd.DataFrame(all_rows, columns=[\"Comment ID\", \"Comment\", \"Response\", \"Theme(s)\"])\n",
    "\n",
    "# Remove any accidental blank rows\n",
    "df = df[df[\"Comment ID\"].str.strip() != \"\"]\n",
    "\n",
    "# df.to_csv(\"NorthMet_TableA3_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 The comment evaluation process used a thematic response approach. Subject matter experts from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the Co-lead Agencies and their consultants rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>area according to the common topic they addres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>theme. Each of the 23 issue areas includes mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>topics addressed by comments. Each comment was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where a comment addressed more than one theme,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0 The comment evaluation process used a thematic response approach. Subject matter experts from\n",
       "0  the Co-lead Agencies and their consultants rev...                                             \n",
       "1  area according to the common topic they addres...                                             \n",
       "2  theme. Each of the 23 issue areas includes mul...                                             \n",
       "3  topics addressed by comments. Each comment was...                                             \n",
       "4  where a comment addressed more than one theme,...                                             "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
