{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42196474",
   "metadata": {},
   "source": [
    "\n",
    "# Comment Classification, Summarization, and Response Notebook\n",
    "\n",
    "End-to-end workflow for training the substantive classifier, summarizing comments, and drafting responses using Azure OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef9ad5",
   "metadata": {},
   "source": [
    "\n",
    "## Workflow Outline\n",
    "1. Load the labeled CSV (`northmet-feis-adequacy-exhibit-a.csv`) and clean columns/labels.\n",
    "2. Parse the Appendix PDF (`014_appendix_a_response_to_comments_on_the_NorthMet_EIS.pdf`) to capture comment/response pairs.\n",
    "3. Train a classifier (Azure embedding + logistic regression) on the labeled dataset.\n",
    "4. Evaluate results and export the classifier for the FastAPI service.\n",
    "5. Demonstrate summarization + response drafting using the appendix dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5bd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755ca51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using root: /Users/samuelsetsofia/dev/law-and-governance-project\n",
      "CSV path: True | PDF path: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Detect repo root (notebook usually runs from /notebooks)\n",
    "ROOT = Path().resolve()\n",
    "if not (ROOT / \"data\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "MODEL_DIR = ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LABELED_PATH = DATA_DIR / \"northmet-feis-adequacy-exhibit-a.csv\"\n",
    "APPENDIX_PATH = DATA_DIR / \"014_appendix_a_response_to_comments_on_the_NorthMet_EIS.pdf\"\n",
    "MODEL_PATH = MODEL_DIR / \"classifier.joblib\"\n",
    "EMBED_CACHE = DATA_DIR / \"comment_embeddings.npy\"\n",
    "\n",
    "print(f\"Using root: {ROOT}\")\n",
    "print(f\"CSV path: {LABELED_PATH.exists()} | PDF path: {APPENDIX_PATH.exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909bc7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env from: /Users/samuelsetsofia/dev/law-and-governance-project/.env\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load environment variables (expects ../.env when running from notebooks)\n",
    "from pathlib import Path as _Path\n",
    "env_path = ROOT / \".env\"\n",
    "load_dotenv(env_path)\n",
    "print(f\"Loaded env from: {env_path}\")\n",
    "\n",
    "required_keys = [\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\",\n",
    "    \"AZURE_OPENAI_CHAT_DEPLOYMENT\",\n",
    "]\n",
    "missing = [key for key in required_keys if not os.getenv(key)]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        f\"Missing Azure OpenAI environment variables: {', '.join(missing)}.\"\n",
    "        \"Create a .env file (see .env.example) or export these variables before running.\"\n",
    "    )\n",
    "\n",
    "API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\")\n",
    "AZURE_CLIENT = AzureOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=API_VERSION,\n",
    ")\n",
    "EMBED_MODEL = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
    "CHAT_MODEL = os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b56c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 4389 (dropped 1166 rows)\n",
      "label\n",
      "non-substantive    2767\n",
      "substantive        1622\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helper: parse labeled CSV with substantive flag\n",
    "cols = [\n",
    "    \"Name of Sender\",\n",
    "    \" Comment\",\n",
    "    \" Issue\",\n",
    "    \"Substantive / Non-Substantive\",\n",
    "    \"Old / New\",\n",
    "    \"Response ID\",\n",
    "    \" RGU Consideration\",\n",
    "]\n",
    "df = pd.read_csv(LABELED_PATH, names=cols, header=0, engine=\"python\")\n",
    "\n",
    "rename_map = {\n",
    "    \"Name of Sender\": \"sender\",\n",
    "    \" Comment\": \"comment\",\n",
    "    \" Issue\": \"issue\",\n",
    "    \"Substantive / Non-Substantive\": \"label_raw\",\n",
    "    \"Old / New\": \"comment_type\",\n",
    "    \"Response ID\": \"response_id\",\n",
    "    \" RGU Consideration\": \"response_text\",\n",
    "}\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Clean\n",
    "original_count = len(df)\n",
    "df = df.dropna(subset=[\"comment\", \"label_raw\"]).copy()\n",
    "df[\"comment\"] = df[\"comment\"].astype(str).str.strip()\n",
    "df = df[df[\"comment\"].str.len() > 20]\n",
    "\n",
    "# Normalize labels\n",
    "label_map = {\n",
    "    \"S\": \"substantive\",\n",
    "    \"NS\": \"non-substantive\",\n",
    "    \"SUBSTANTIVE\": \"substantive\",\n",
    "    \"NON-SUBSTANTIVE\": \"non-substantive\",\n",
    "}\n",
    "df[\"label_key\"] = df[\"label_raw\"].astype(str).str.strip().str.upper()\n",
    "df = df[df[\"label_key\"].isin(label_map)].copy()\n",
    "df[\"label\"] = df[\"label_key\"].map(label_map)\n",
    "df[\"response_text\"] = df[\"response_text\"].fillna(\"\").str.strip()\n",
    "df = df.drop(columns=[\"label_raw\", \"label_key\"])\n",
    "\n",
    "print(f\"Rows after cleaning: {len(df)} (dropped {original_count - len(df)} rows)\")\n",
    "print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fee5685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 33 appendix comment/response pairs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>response_text</th>\n",
       "      <th>themes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2981</td>\n",
       "      <td>Spill prevention is an important part of the m...</td>\n",
       "      <td>To guard against possible adverse effects from...</td>\n",
       "      <td>spilled ore, PolyMet plans the couplings and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2982</td>\n",
       "      <td>Pages 5-50 forward describe how the company ha...</td>\n",
       "      <td>Mine waste rock would be sorted and stored its...</td>\n",
       "      <td>into four categories based on not produce acid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2983</td>\n",
       "      <td>Page 5-157, Section 5.2.2.3.3, 2nd Paragraph: ...</td>\n",
       "      <td>FEIS Section 5.2.14.2.3, which expands upon SD...</td>\n",
       "      <td>the discussion from the Hydrometallurgical Res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                       comment_text  \\\n",
       "0       2981  Spill prevention is an important part of the m...   \n",
       "1       2982  Pages 5-50 forward describe how the company ha...   \n",
       "2       2983  Page 5-157, Section 5.2.2.3.3, 2nd Paragraph: ...   \n",
       "\n",
       "                                       response_text  \\\n",
       "0  To guard against possible adverse effects from...   \n",
       "1  Mine waste rock would be sorted and stored its...   \n",
       "2  FEIS Section 5.2.14.2.3, which expands upon SD...   \n",
       "\n",
       "                                              themes  \n",
       "0  spilled ore, PolyMet plans the couplings and l...  \n",
       "1  into four categories based on not produce acid...  \n",
       "2  the discussion from the Hydrometallurgical Res...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Parser for Appendix PDF comment/response pairs\n",
    "\n",
    "def parse_appendix_pdf(pdf_path: Path) -> pd.DataFrame:\n",
    "    records: List[Dict[str, str]] = []\n",
    "    current = None\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            words = page.extract_words(use_text_flow=True)\n",
    "            for idx, word in enumerate(words):\n",
    "                text = word[\"text\"].strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "                x0 = word[\"x0\"]\n",
    "                if text.isdigit() and len(text) >= 3 and x0 < 110:\n",
    "                    lookahead = words[idx + 1 : idx + 6]\n",
    "                    if not any(\"Comment\" in w.get(\"text\", \"\") for w in lookahead):\n",
    "                        continue\n",
    "                    if current and (current[\"comment_text\"].strip() or current[\"response_text\"].strip()):\n",
    "                        records.append(current)\n",
    "                    current = {\n",
    "                        \"comment_id\": text,\n",
    "                        \"comment_text\": \"\",\n",
    "                        \"response_text\": \"\",\n",
    "                        \"themes\": \"\",\n",
    "                    }\n",
    "                    continue\n",
    "                if current is None:\n",
    "                    continue\n",
    "                if x0 < 140:\n",
    "                    # comment numbering / artifacts\n",
    "                    continue\n",
    "                if x0 < 360:\n",
    "                    current[\"comment_text\"] += \" \" + text\n",
    "                elif x0 < 540:\n",
    "                    current[\"response_text\"] += \" \" + text\n",
    "                else:\n",
    "                    current[\"themes\"] += \" \" + text\n",
    "    if current and (current[\"comment_text\"].strip() or current[\"response_text\"].strip()):\n",
    "        records.append(current)\n",
    "\n",
    "    appendix_df = pd.DataFrame(records)\n",
    "    appendix_df = appendix_df[appendix_df[\"comment_text\"].str.contains(\"Comment #\", na=False)].copy()\n",
    "    appendix_df[\"comment_text\"] = (\n",
    "        appendix_df[\"comment_text\"]\n",
    "        .str.replace(r\"Comment #\\s*\\d+\\.\\s*\", \"\", regex=True)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    appendix_df[\"response_text\"] = appendix_df[\"response_text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    appendix_df[\"themes\"] = appendix_df[\"themes\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return appendix_df\n",
    "\n",
    "appendix_df = parse_appendix_pdf(APPENDIX_PATH)\n",
    "print(f\"Parsed {len(appendix_df)} appendix comment/response pairs\")\n",
    "appendix_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d168f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 3511 | Test rows: 878\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/test split for classifier\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "print(f\"Train rows: {len(train_df)} | Test rows: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6688cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings from Azure OpenAI (may take several minutes)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def embed_batch(texts: List[str]) -> List[List[float]]:\n",
    "    response = AZURE_CLIENT.embeddings.create(input=texts, model=EMBED_MODEL)\n",
    "    return [item.embedding for item in response.data]\n",
    "\n",
    "\n",
    "def build_embeddings(texts: List[str], batch_size: int = 64) -> np.ndarray:\n",
    "    vectors: List[List[float]] = []\n",
    "    for start in range(0, len(texts), batch_size):\n",
    "        batch = texts[start : start + batch_size]\n",
    "        vectors.extend(embed_batch(batch))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n",
    "if EMBED_CACHE.exists():\n",
    "    print(\"Loading cached embeddings...\")\n",
    "    cached = np.load(EMBED_CACHE, allow_pickle=True).item()\n",
    "    train_X = cached[\"train_X\"]\n",
    "    test_X = cached[\"test_X\"]\n",
    "else:\n",
    "    print(\"Computing embeddings from Azure OpenAI (may take several minutes)...\")\n",
    "    train_X = build_embeddings(train_df[\"comment\"].tolist())\n",
    "    test_X = build_embeddings(test_df[\"comment\"].tolist())\n",
    "    np.save(EMBED_CACHE, {\"train_X\": train_X, \"test_X\": test_X})\n",
    "\n",
    "train_y = train_df[\"label\"].values\n",
    "test_y = test_df[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8a0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "non-substantive       0.87      0.87      0.87       554\n",
      "    substantive       0.77      0.77      0.77       324\n",
      "\n",
      "       accuracy                           0.83       878\n",
      "      macro avg       0.82      0.82      0.82       878\n",
      "   weighted avg       0.83      0.83      0.83       878\n",
      "\n",
      "[[480  74]\n",
      " [ 74 250]]\n",
      "Saved classifier to /Users/samuelsetsofia/dev/law-and-governance-project/models/classifier.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train classifier (logistic regression on embeddings)\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "pred_y = clf.predict(test_X)\n",
    "pred_probs = clf.predict_proba(test_X)\n",
    "\n",
    "print(classification_report(test_y, pred_y))\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "\n",
    "joblib.dump(clf, MODEL_PATH)\n",
    "print(f\"Saved classifier to {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62435b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions for summarization + response generation\n",
    "def summarize_comment(comment: str, response: str | None = None) -> str:\n",
    "    if response:\n",
    "        prompt = (\n",
    "            \"Summarize the public comment and note how the existing agency response addresses it.\\n\"\n",
    "            f\"Comment: {comment}\\n\"\n",
    "            f\"Agency Response: {response}\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"Summarize the public comment in 2 sentences, highlighting issues and requested actions.\\n\"\n",
    "            f\"Comment: {comment}\"\n",
    "        )\n",
    "    completion = AZURE_CLIENT.responses.create(\n",
    "        model=CHAT_MODEL,\n",
    "        input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_output_tokens=200,\n",
    "    )\n",
    "    return completion.output[0].content[0].text.strip()\n",
    "\n",
    "\n",
    "def draft_response(comment: str, summary: str, label: str, response: str | None = None) -> str:\n",
    "    prompt_lines = [\n",
    "        \"Draft a short agency response (<=150 words).\",\n",
    "        \"Acknowledge the comment, reference commitments, and stay neutral.\",\n",
    "        f\"Summary: {summary}\",\n",
    "        f\"Classification: {label}\",\n",
    "        f\"Comment: {comment}\",\n",
    "    ]\n",
    "    if response:\n",
    "        prompt_lines.append(f\"Existing response: {response}\")\n",
    "        prompt_lines.append(\"Only add new info if necessary; otherwise reiterate commitments succinctly.\")\n",
    "\n",
    "    completion = AZURE_CLIENT.responses.create(\n",
    "        model=CHAT_MODEL,\n",
    "        input=[{\"role\": \"user\", \"content\": \"\\n\".join(prompt_lines)}],\n",
    "        temperature=0.4,\n",
    "        max_output_tokens=300,\n",
    "    )\n",
    "    return completion.output[0].content[0].text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e7c978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>response_excerpt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2981</td>\n",
       "      <td>Spill prevention is an important part of the m...</td>\n",
       "      <td>To guard against possible adverse effects from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2982</td>\n",
       "      <td>Pages 5-50 forward describe how the company ha...</td>\n",
       "      <td>Mine waste rock would be sorted and stored its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2983</td>\n",
       "      <td>Page 5-157, Section 5.2.2.3.3, 2nd Paragraph: ...</td>\n",
       "      <td>FEIS Section 5.2.14.2.3, which expands upon SD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id                                            comment  \\\n",
       "0       2981  Spill prevention is an important part of the m...   \n",
       "1       2982  Pages 5-50 forward describe how the company ha...   \n",
       "2       2983  Page 5-157, Section 5.2.2.3.3, 2nd Paragraph: ...   \n",
       "\n",
       "                                    response_excerpt  \n",
       "0  To guard against possible adverse effects from...  \n",
       "1  Mine waste rock would be sorted and stored its...  \n",
       "2  FEIS Section 5.2.14.2.3, which expands upon SD...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Demo across both datasets (set demo_mode=True to skip Azure calls)\n",
    "demo_mode = True\n",
    "results = []\n",
    "\n",
    "sample_comments = appendix_df.head(3).to_dict(orient=\"records\")\n",
    "for item in sample_comments:\n",
    "    if demo_mode:\n",
    "        results.append(\n",
    "            {\n",
    "                \"comment_id\": item[\"comment_id\"],\n",
    "                \"comment\": item[\"comment_text\"][:200] + \"...\",\n",
    "                \"response_excerpt\": item[\"response_text\"][:200] + \"...\",\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        summary = summarize_comment(item[\"comment_text\"], item[\"response_text\"])\n",
    "        probs = clf.predict_proba(np.array(embed_batch([item[\"comment_text\"]])))\n",
    "        label_idx = int(np.argmax(probs[0]))\n",
    "        label = clf.classes_[label_idx]\n",
    "        generated = draft_response(item[\"comment_text\"], summary, label, item[\"response_text\"])\n",
    "        results.append(\n",
    "            {\n",
    "                \"comment_id\": item[\"comment_id\"],\n",
    "                \"summary\": summary,\n",
    "                \"label\": label,\n",
    "                \"probability\": float(probs[0][label_idx]),\n",
    "                \"generated_response\": generated,\n",
    "            }\n",
    "        )\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa0dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
